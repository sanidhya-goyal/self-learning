# Design: Rate Limiter

In a **DDoS attack**, a server is flooded with thousands or millions of requests, causing the server to crash.

To prevent this, we use a **Rate Limiter** to limit how frequently an API can be called by a particular client/user.

---

## Purpose

**Goal:** Control the rate of requests hitting the server  
**Typical use case:** Allow `N` requests per user per time window  
**HTTP Status Code on limit exceeded:** `429 Too Many Requests`

---

## Algorithms for Rate Limiting

### 1. Token Bucket

- A bucket holds tokens (e.g., max 4).
- Tokens are added at a fixed rate by a refiller.
- Each request consumes one token.

**Flow:**

```
Request → Is token available?
↳ Yes → consume token → allow
↳ No → deny (HTTP 429)
```

**Example Configuration:**

Rule -
For each user say 3 token per minute
And for POST api say 1 token  per minute

So user stores a counter

Post request -> U : { counter : 2, time : “10:01:00” }

Post request -> U : { counter : 1, time : “10:01:25” }

Post request -> U : { counter : 0, time : “10:01:35” }

Post request -> U :  -> denied
Whenever the request is denied the HTTP code is 429 - Excessive request in a time limit


At 10:03
Capacity was 3 so after 3 minutes all the tokens are refilled


**Implementation:** Use **counters**

---

### 2. Leaky Bucket

- A bucket (queue) of fixed size holds incoming requests.
- Requests are **processed at a constant rate** (the leak rate).
- New requests are added only if the bucket is not full.

**Flow:**

```
Request → Is queue full?
↳ No → enqueue → process at constant rate
↳ Yes → deny (HTTP 429)
```

**Disadvantage:**  Traffic may vary at different times, e.g. amazon prime is more flooded at night than day
So this will fail in that scenario

**Implementation:** Use a **queue**

---

### 3. Fixed Window Counter

- Fixed time windows (e.g., 5 minutes)
- Each window has a counter to count requests.

**Flow:**

```
Time bucket → maintain count
Within a window: allow up to N requests
```


**Disadvantage:**
If requests arrive at the boundary of two windows, we may exceed the intended rate limit.
e.g. 

8:05      8:10      8:15

Window1: 8:05–8:10
Window2: 8:10–8:15

3 requests at 8:09
3 requests at 8:11 → total 6 in short span


---

### 4. Sliding Window Log

- Maintain a **log of timestamps** of each request.
- When a request comes in, prune outdated logs.

**Flow:**

```
Keep log of each request
Remove logs older than (current time - window)
Check if log size < limit → allow or deny
```

**Example:**

5 min time frame

When the window slides there’s no fixed slice
Assume 3 request per minute is allowed configuration
And we log every request

```
Initially size is 0

1st request at 10:00:10		window started  	size = 1

2nd request at 10:00:20		valid request		size = 2

3rd request at 10:00:40		valid request		size = 3

4th request at 10:00:45		declined request	but this will be logged

5th request at 10:01:20		so now since the window slides, the requests started to be removed from log from the top
```

**Disadvantage:**
Even if the request is denied, we are storing the log


---

### 5. Sliding Window Counter

- It actually combines 2 functionalities **Fixed Window Counter** + **Sliding Window Log**
- Counts requests in **current + partial previous window**

**Example:**

```
    8:00:50     8:01:50
          |   |       |
8:00        8:01        8:02        8:03        8:04

```

- Config: 5 requests/min
- Current time: 8:01:50

The goal is to find the number of request present in this window

Assume that the current window frame is 8:00:50 - 8:01:50 (i.e. the current time is 8:01:50)
From 8:01:00–8:01:50 → 2 requests are there\
And for the other 10 secs of previous frame 

For 8:00:50 -> 8:01:00

In 60 intervals   ->    6 requests are there
For 10 intervals  -> 
```
6/60  *  10
```
**Effective count:** 2 + ( 6/60   *   10 ) = 2 + 1 = 3 -> allowed


---

## Architecture Components

### Basic Architecture

- For bucket we need Counter
- We need in-memory so Redis 
- Everything is stored in config file, like window size, counter value, etc. so we will also need config

```
Client 		——> 		Server
```

2 possibilities - If there is no gateway in the mid, then server will have rate limiter algorithm

Or else

There could be Api gateway in between, where we can put rate limiter algorithm

```
Client 		——> 	RateLimiter		——>   Server
                        |
                        |
                        ——> Redis
```

For config files we need to have a Cache in order to load configs into that
The rate limiter will return success or 429


Possible Questions :
1. There can be distributed ( more than 1 ) ratelimiter
So we can use redis, redis is centralised data store, so every rate limiter can connect to redis

2. Redis doesn’t follow atomicity

For e.g. Let’s say 2 rate limiters connecting to centralised redis, and the value of counter is 2
so, to solve this, we can add atomicity to the Redis but it will add little bit latency






