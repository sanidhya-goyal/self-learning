# Design: Key-Value Database

Example -
DynamoDB
Shopping cart systems (e.g., Amazon "Add to Cart")

---

### Design Goals

1. Scalability
2. Decentralization
3. Eventual Consistency

---

### High-Level Design Steps

1. Partitioning (Consistent Hashing)
2. Replication
3. Get and Put Operations
4. Data Versioning
5. Gossip Protocol
6. Merkle Tree for Synchronization

---

### 1. Partitioning

- In-memory hash table: `Key → Value`
- With billions of users distributed across regions, single-server memory is insufficient, for scalability we have 
'consistency hashing' for partition

#### Solution: Consistent Hashing

- Circular ring of hash space and we create partition then we assign range of keys to servers
- Divide hash range among servers:
    - Example:
    ```
    1-50    → S1 
    51-100  → S2 
    101-150 → S3 
    151-200 → S4
    ```
    Let's take an example of some data with generated key = 45, so the data will be stored at S1
- Virtual Servers : We have virtual nodes where we keep replicas of servers at multiple positions

---

### 2. Decentralization (Replication)

In case of failure, the system should not get down

- **Replication Number `N`**: default = 3
- **Coordinator**: The key falls in whichever range, whatever server takes care of it is known as coordinator.
- Now, to perform replication, it will move clockwise and find N-1 servers\
Since N = 3 and S1 server is already added as coordinator, N-1 server will have a copy\
So, next 2 servers encountered will store the copy of same data
- We have a Preference List\
**Preference List**:
For key range 1 - 50
    - S1 (Primary) with highest preference, then S2 and S3 (Secondary Replicas)\
    Whenever a key comes, it falls into one of the range of a server (in the ring of coinsistent hashing),\
whichever range that server falls into, is called as coordinator that will be on the top of the preference list,\
whatever is default or configured value of N (replicas number), put the replicas copy in N-1 servers,\
(as the coordinator is already having the data)
    - Replicas spread across **different data centers**.
    - Skip same virtual node in preference list (S1 is the coordinator then we will keep the replicas in S2 and S3)

---

### 3. Get and Put Operations

#### a. PUT Operation

- `PUT(key=45, value="Car")`
- Coordinator: S1
- Preference list: S1, S2, S3
- Write to S1, replicate asynchronously to S2 and S3.

**Write Consistency (`W`)** -> As soon as we get 'W' success responses from replicas, we will consider our 'PUT' request successful
- If `W = 1`, success is returned after 1 replica acknowledges write.
- Follows rule:  

```
R + W > N
```
Basically whenever client hits a put request, we will send the Success response only when, 
N is Replica Number
W is minimum severs that needs to store the copy of value


#### b. GET Operation

- Two types of Load Balancers:
1. **Generic** (request goes to any node):
    - Node may **forward** to coordinator (called "hopping").
    - Higher latency, simple.
2. **Partition-Aware**:
    - Sends request directly to the coordinator.
    - Lower latency.

**Read Consistency (`R`)**
- Coordinator queries replicas.
- If `R = 2`, waits for 2 replicas to respond.

The coordinator will ask the 'R' number of replicas for the copy of value, it will return the reponse

---

### 4. Data Versioning

#### Problem

- Multiple replicas may hold different versions of the same key due to failures or network delays.

#### Vector Clocks

- Each version carries a **vector clock**:
- Puts a list of Server, counter for each object / data of the key.
```
[S1, counter1], [S2, counter2], ...
```


#### Example Scenario

- Key = 45
- Preference list: S1, S2, S3 (S1 is the coordinator)
- Writes:
- `PUT(k1, "CAR") → [S1, 1]` (async propagation to S2 and S3)
- `PUT(k1, "CART") → [S1, 2]` (Updates the version to '2' and replicates to S2 and S3)
Assume S1 is down, 2 parallel requests come, also assume there's partition between S2 and S3
- `PUT(k1, "CARM") → [S1, 2][S2, 1]` (Handled by S2)
- `PUT(k1, "CARR") → [S1, 2][S3, 1]` (handled by S3)


GET (K1) --> 
S1 will ask the replicas 
```
- S1 --> CART [S1, 2]
- S2 --> CARM [S1,2] [S2,1]
- S3 --> CARR [S1,2] [S3,1]
```

So in this case both the data ( S2's and S3's) will be returned to the client and the client will have 'Last Write Wins' algorithm,
which will resolve the conflict and also will update the servers
So, again a PUT (K1)
` --> [S1,3] [S2,1] [S3,1] ` and this will be replicated to S2 and S3 as well


#### Eventual Consistency : At some point we may get some stale data, but over the time hitting get again will give us the latest data

#### Conflict Resolution

- During `GET`, conflicting versions returned to client.
- **Client-side resolution**: e.g., "Last write wins"
- After resolution, updated version is written back with merged vector clock.

---

### 5. Gossip Protocol

- Ensures decentralized membership and failure detection.

#### Mechanism

- Each server:
- Maintains a member list.
- Periodically sends **heartbeat** to random nodes.
- Shares range responsibility and up/down status.

#### Failure Detection

- A node is marked down **only if multiple nodes agree** it is unreachable.

---

### 6. Merkle Tree

- Used to **synchronize data between replicas**.

#### Example

- S1 (coordinator): Key range `1-1000`
- S2 (replica): Same range
- Data inconsistency in subrange `500-700`

#### How it Works

- Leaf node will have Keys
- Their parent will have hash of their keys
- So if the hash is different, than there is some inconsistency, then it will check the left and right subtree, 
and in this way it can identify that which node is having mismatched data

![Merkle Tree.jpeg](Images/Merkle%20Tree.jpeg)

---

### Eventual Consistency

- At any moment, a `GET` may return **stale data**.
- Over time, due to replication and gossip, all replicas **converge** to the latest value.
- **CAP Theorem**:
- Compromises **Consistency**
- Prioritizes **Availability** and **Partition Tolerance**

---
